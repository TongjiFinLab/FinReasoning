import json
import re
import os
from pathlib import Path
from typing import List, Dict, Any, Optional
from .utils import get_openai_client, setup_logger

logger = setup_logger("Consistency_Evaluator")

try:
    from bert_score import score as bert_score
    BERTSCORE_AVAILABLE = True
except ImportError:
    BERTSCORE_AVAILABLE = False
    logger.warning("bert-score æœªå®‰è£…ï¼ŒBERTScoreæŒ‡æ ‡å°†æ— æ³•è®¡ç®—ã€‚è¯·è¿è¡Œ: pip install bert-score")

try:
    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    logger.warning("scikit-learn æœªå®‰è£…ï¼Œåˆ†ç±»æŒ‡æ ‡å°†æ— æ³•è®¡ç®—ã€‚è¯·è¿è¡Œ: pip install scikit-learn")

try:
    from sentence_transformers import SentenceTransformer, util
    SIMCSE_AVAILABLE = True
except ImportError:
    SIMCSE_AVAILABLE = False
    logger.warning("sentence-transformers æœªå®‰è£…ï¼ŒSimCSEæŒ‡æ ‡å°†æ— æ³•è®¡ç®—ã€‚è¯·è¿è¡Œ: pip install sentence-transformers")


class ErrorDetectionEvaluator:
    """é”™è¯¯è¯†åˆ«èƒ½åŠ›è¯„ä¼°å™¨"""
    
    def __init__(self, api_key: str, base_url: str, model: str = "deepseek-chat", judge_model: str = None):
        """åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            api_key: APIå¯†é’¥
            base_url: APIåŸºç¡€URL
            model: æ¨¡å‹åç§°
            judge_model: è£åˆ¤æ¨¡å‹åç§°
        """
        self.client = get_openai_client(api_key, base_url)
        self.model = model
        self.judge_model = judge_model
        
        # SimCSEæ¨¡å‹
        if SIMCSE_AVAILABLE:
            try:
                self.simcse_model = SentenceTransformer("BAAI/bge-base-zh-v1.5")
                # self.simcse_model = "BAAI/bge-base-zh-v1.5"
            except Exception as e:
                logger.warning(f"åŠ è½½SimCSEæ¨¡å‹å¤±è´¥: {e}")
                self.simcse_model = None
        else:
            self.simcse_model = None
    
    def load_qa_dataset(self, qa_file: str) -> List[Dict[str, Any]]:
        """åŠ è½½QAæ•°æ®é›†"""
        qa_dataset = []
        try:
            with open(qa_file, "r", encoding="utf-8") as f:
                content = f.read().strip()
                if content.startswith('[') and content.endswith(']'):
                    qa_dataset = json.loads(content)
                else:
                    for line in content.splitlines():
                        if line.strip():
                            qa_dataset.append(json.loads(line.strip()))
        except Exception as e:
            logger.error(f"åŠ è½½æ–‡ä»¶ {qa_file} å¤±è´¥: {e}")
            qa_dataset = []
            try:
                with open(qa_file, "r", encoding="utf-8") as f:
                    for line in f:
                        if line.strip():
                            try:
                                qa_dataset.append(json.loads(line.strip()))
                            except:
                                continue
            except Exception as e2:
                logger.error(f"é€è¡Œé‡è¯•åŠ è½½å¤±è´¥: {e2}")
        
        return qa_dataset
    
    def call_model(self, prompt: str, max_retries: int = 3) -> Optional[Dict[str, Any]]:
        """è°ƒç”¨æ¨¡å‹API"""
        for retry in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {
                            'role': 'system',
                            'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èäº‹å®æ ¸æŸ¥ä¸“å®¶ï¼Œæ“…é•¿è¯†åˆ«æ–‡æœ¬ä¸­çš„äº‹å®é”™è¯¯ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ç»“æœã€‚'
                        },
                        {
                            'role': 'user',
                            'content': prompt
                        }
                    ],
                    temperature=0.3,
                    max_tokens=8000
                )
                
                full_response = response.choices[0].message.content
                response_text = full_response.strip()
                
                # å»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                if response_text.startswith('```json'):
                    response_text = response_text[7:]
                if response_text.startswith('```'):
                    response_text = response_text[3:]
                if response_text.endswith('```'):
                    response_text = response_text[:-3]
                
                result = json.loads(response_text.strip())
                return result
                
            except json.JSONDecodeError as e:
                if retry < max_retries - 1:
                    print(f"  âš ï¸ JSONè§£æå¤±è´¥ï¼Œé‡è¯•ä¸­... ({retry + 2}/{max_retries})")
                else:
                    print(f"  âœ— JSONè§£æå¤±è´¥: {e}")
                    print(f"  åŸå§‹å“åº”: {full_response[:500]}...")
            except Exception as e:
                if retry < max_retries - 1:
                    print(f"  âš ï¸ APIè°ƒç”¨å¤±è´¥ï¼Œé‡è¯•ä¸­... ({retry + 2}/{max_retries})")
                else:
                    print(f"  âœ— APIè°ƒç”¨å¤±è´¥: {e}")
        
        return None
    
    def call_judge_model(self, prompt: str, max_retries: int = 3) -> Optional[Dict[str, Any]]:
        """è°ƒç”¨è¯„åˆ¤æ¨¡å‹"""
        if not self.judge_model:
            return None
            
        for retry in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.judge_model,
                    messages=[
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯ä¸€åä¸“ä¸šçš„è¯„ä¼°ä¸“å®¶ï¼Œæ“…é•¿ä»å¤šä¸ªç»´åº¦è¯„ä¼°æ–‡æœ¬è´¨é‡ã€‚è¯·ä»”ç»†åˆ†æç»™å®šçš„å†…å®¹ï¼Œå¹¶æŒ‰ç…§è¦æ±‚ç»™å‡ºè¯¦ç»†çš„è¯„ä¼°ç»“æœã€‚"
                        },
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3,
                    max_tokens=4000
                )
                text = (response.choices[0].message.content or "").strip()
                # æå– JSON
                start = text.find('{')
                if start == -1:
                    continue
                depth = 0
                end = start
                for i in range(start, len(text)):
                    if text[i] == '{':
                        depth += 1
                    elif text[i] == '}':
                        depth -= 1
                        if depth == 0:
                            end = i + 1
                            break
                if depth == 0:
                    try:
                        return json.loads(text[start:end])
                    except json.JSONDecodeError:
                        pass
                # å°è¯•å»é™¤ markdown
                clean = text.replace('```json', '').replace('```', '').strip()
                start = clean.find('{')
                if start != -1:
                    depth = 0
                    end = start
                    for i in range(start, len(clean)):
                        if clean[i] == '{':
                            depth += 1
                        elif clean[i] == '}':
                            depth -= 1
                            if depth == 0:
                                end = i + 1
                                break
                    if depth == 0:
                        try:
                            return json.loads(clean[start:end])
                        except json.JSONDecodeError:
                            pass
            except Exception as e:
                if retry < max_retries - 1:
                    logger.warning(f"    âš ï¸ è£åˆ¤æ¨¡å‹APIè°ƒç”¨å¤±è´¥ï¼Œé‡è¯• ({retry + 2}/{max_retries}): {e}")
                else:
                    logger.error(f"    âœ— è£åˆ¤æ¨¡å‹APIè°ƒç”¨å¤±è´¥: {e}")
        return None

    def build_judge_prompt(
        self,
        model_full_output: Dict[str, Any],
        reference_answer: Dict[str, Any]
    ) -> str:
        """æ„å»ºè¯„ä¼° promptï¼šè¾“å…¥æ¨¡å‹å…¨éƒ¨è¾“å‡ºä¸å‚è€ƒç­”æ¡ˆ"""
        model_json = json.dumps(model_full_output, ensure_ascii=False, indent=2)
        ref_json = json.dumps(reference_answer, ensure_ascii=False, indent=2)

        return f"""è¯·æ ¹æ®ä»¥ä¸‹æ¨¡å‹è¾“å‡ºçš„å…¨éƒ¨å†…å®¹ä¸å‚è€ƒç­”æ¡ˆï¼Œå¯¹æ¨¡å‹è¿›è¡Œç»¼åˆè¯„ä¼°ã€‚

        ## æ¨¡å‹çš„å®Œæ•´è¾“å‡ºï¼ˆåŒ…å«é”™è¯¯åˆ¤å®šã€é”™è¯¯åˆ—è¡¨ã€é”™è¯¯ç†ç”±ã€ä¿®æ­£æ–‡æœ¬ï¼‰

        ```json
        {model_json}
        ```

        ## å‚è€ƒç­”æ¡ˆï¼ˆåŒ…å«é”™è¯¯åˆ—è¡¨ã€é”™è¯¯è§£é‡Šã€ä¿®æ­£æ–‡æœ¬ï¼‰

        ```json
        {ref_json}
        ```

        ## è¯„ä¼°ç»´åº¦è¯´æ˜

        è¯·ä»ä»¥ä¸‹å››ä¸ªç»´åº¦åˆ†åˆ«å¯¹ **é”™è¯¯ç†ç”±** å’Œ **ä¿®æ­£å†…å®¹** è¿›è¡Œæ‰“åˆ†ï¼ˆæ¯é¡¹ 0â€“10 åˆ†ï¼Œä¿ç•™ 1 ä½å°æ•°ï¼‰ï¼Œå¹¶ä¸ºæ¯ä¸ªç»´åº¦ç»™å‡ºè¯¦ç»†çš„è¯„ä¼°è¯´æ˜ï¼š

        1. **æ­£ç¡®æ€§**ï¼šè¯„ä¼°æ¨¡å‹æ˜¯å¦å‡†ç¡®ç†è§£äº†é”™è¯¯ã€æ˜¯å¦ç»™å‡ºäº†æ­£ç¡®çš„é”™è¯¯åŸå› æˆ–ä¿®æ­£ã€‚è€ƒå¯Ÿå†…å®¹ä¸å‚è€ƒç­”æ¡ˆçš„å»åˆç¨‹åº¦ã€‚
        2. **æ¨ç†ä¸¥å¯†æ€§**ï¼šæ£€æŸ¥æ¨ç†è¿‡ç¨‹æ˜¯å¦æ¸…æ™°ã€é€»è¾‘æ˜¯å¦è¿è´¯ã€è®ºè¯æ˜¯å¦å……åˆ†ï¼Œä¿®æ­£æ˜¯å¦åŸºäºåˆç†çš„é€»è¾‘ä¾æ®ã€‚
        3. **ç›¸å…³æ€§**ï¼šè¯„ä¼°æ˜¯å¦é’ˆå¯¹å…·ä½“çš„é”™è¯¯è¿›è¡Œè§£é‡Šæˆ–ä¿®æ­£ï¼Œæ˜¯å¦ä¸é”™è¯¯å†…å®¹ç›´æ¥ç›¸å…³ï¼Œæ˜¯å¦å­˜åœ¨æ— å…³æˆ–åç¦»ä¸»é¢˜çš„å†…å®¹ã€‚
        4. **å®Œæ•´æ€§**ï¼šæ£€æŸ¥æ˜¯å¦è¦†ç›–äº†æ‰€æœ‰é‡è¦çš„é”™è¯¯ç‚¹æˆ–ä¿®æ­£ç‚¹ï¼Œæ˜¯å¦é—æ¼äº†å½±å“ç»“è®ºçš„å…³é”®ä¿¡æ¯ã€‚

        ## è¯„ä¼°å¯¹è±¡

        1. **æ¨¡å‹è¾“å‡ºçš„é”™è¯¯ç†ç”±ï¼ˆexplanationï¼‰**ï¼šç»“åˆæ¨¡å‹è¯†åˆ«çš„é”™è¯¯åˆ—è¡¨ä¸ç†ç”±è¿›è¡Œå››ç»´è¯„ä¼°ã€‚
        2. **æ¨¡å‹è¾“å‡ºçš„ä¿®æ­£å†…å®¹ï¼ˆcorrected_textï¼‰**ï¼šå¯¹ä¿®æ­£åçš„å®Œæ•´æ–‡æœ¬è¿›è¡Œå››ç»´è¯„ä¼°ã€‚

        ## è¾“å‡ºæ ¼å¼è¦æ±‚

        è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ï¼Œä¸å¾—æ·»åŠ é¢å¤–è¯´æ˜ï¼š

        {{
        "error_reasoning": {{
            "correctness": {{ "score": åˆ†æ•°, "explanation": "æ­£ç¡®æ€§è¯„ä¼°è¯´æ˜" }},
            "reasoning_rigor": {{ "score": åˆ†æ•°, "explanation": "æ¨ç†ä¸¥å¯†æ€§è¯„ä¼°è¯´æ˜" }},
            "relevance": {{ "score": åˆ†æ•°, "explanation": "ç›¸å…³æ€§è¯„ä¼°è¯´æ˜" }},
            "completeness": {{ "score": åˆ†æ•°, "explanation": "å®Œæ•´æ€§è¯„ä¼°è¯´æ˜" }}
        }},
        "corrected_text": {{
            "correctness": {{ "score": åˆ†æ•°, "explanation": "æ­£ç¡®æ€§è¯„ä¼°è¯´æ˜" }},
            "reasoning_rigor": {{ "score": åˆ†æ•°, "explanation": "æ¨ç†ä¸¥å¯†æ€§è¯„ä¼°è¯´æ˜" }},
            "relevance": {{ "score": åˆ†æ•°, "explanation": "ç›¸å…³æ€§è¯„ä¼°è¯´æ˜" }},
            "completeness": {{ "score": åˆ†æ•°, "explanation": "å®Œæ•´æ€§è¯„ä¼°è¯´æ˜" }}
        }}
        }}

        è¯·å¼€å§‹è¯„ä¼°ï¼š"""

    def evaluate_error_list(self, ground_truth_errors: List[Dict], predicted_errors: List[Dict]) -> Dict[str, float]:
        """è¯„ä¼°é”™è¯¯åˆ—ä¸¾ï¼ˆä»…ä¿ç•™å¥å­çº§æŒ‡æ ‡ï¼Œåˆ é™¤å­—ç¬¦çº§æŒ‡æ ‡ï¼‰"""
        def normalize_text(text: str) -> str:
            if not text:
                return ''
            return re.sub(r'[\sï¼Œã€‚ã€ï¼›ï¼šï¼ï¼Ÿ""\'ï¼ˆï¼‰ã€ã€‘]', '', text)

        # å¥å­çº§
        gt_sentences, pred_sentences = [], []
        for err in ground_truth_errors:
            loc = err.get('error_location', '') or err.get('context', '')
            if loc and loc.strip():
                gt_sentences.append(loc.strip())
        for err in predicted_errors:
            loc = err.get('error_location', '') or err.get('context', '')
            if loc and loc.strip():
                pred_sentences.append(loc.strip())

        if gt_sentences or pred_sentences:
            matched_gt, matched_pred = set(), set()
            for i, gt_sent in enumerate(gt_sentences):
                gt_norm = normalize_text(gt_sent)
                best_idx, best_sim = -1, 0.0
                for j, pred_sent in enumerate(pred_sentences):
                    if j in matched_pred:
                        continue
                    pred_norm = normalize_text(pred_sent)
                    is_contained = gt_norm in pred_norm or pred_norm in gt_norm
                    if gt_norm == pred_norm:
                        sim = 1.0
                    elif is_contained:
                        sim = 1.0
                    else:
                        gt_chars, pred_chars = set(gt_norm), set(pred_norm)
                        if gt_chars or pred_chars:
                            inter = len(gt_chars & pred_chars)
                            union = len(gt_chars | pred_chars)
                            sim = inter / union if union > 0 else 0.0
                        else:
                            sim = 0.0
                    if (is_contained or sim > 0.8) and sim > best_sim:
                        best_sim, best_idx = sim, j
                if best_idx >= 0:
                    matched_gt.add(i)
                    matched_pred.add(best_idx)
            sentence_precision = len(matched_pred) / len(pred_sentences) if pred_sentences else 0.0
            sentence_recall = len(matched_gt) / len(gt_sentences) if gt_sentences else 0.0
            sentence_f1 = (
                2 * sentence_precision * sentence_recall / (sentence_precision + sentence_recall)
                if (sentence_precision + sentence_recall) > 0
                else 0.0
            )
        else:
            sentence_f1 = sentence_precision = sentence_recall = 1.0

        return {
            'sentence_f1': sentence_f1,
            'sentence_precision': sentence_precision,
            'sentence_recall': sentence_recall,
            'num_gt_errors': len(ground_truth_errors),
            'num_pred_errors': len(predicted_errors),
        }

    def evaluate_explanation(self, ground_truth: str, prediction: str) -> Dict[str, float]:
        """è¯„ä¼°é”™è¯¯è§£é‡Š"""
        metrics: Dict[str, float] = {}

        if BERTSCORE_AVAILABLE:
            try:
                P, R, F1 = bert_score([prediction], [ground_truth], lang='zh', verbose=False)
                metrics['bertscore_f1'] = float(F1[0].item())
                metrics['bertscore_precision'] = float(P[0].item())
                metrics['bertscore_recall'] = float(R[0].item())
            except Exception as e:
                print(f"  âš ï¸ BERTScoreè®¡ç®—å¤±è´¥: {e}")
                metrics['bertscore_f1'] = metrics['bertscore_precision'] = metrics['bertscore_recall'] = 0.0
        else:
            metrics['bertscore_f1'] = metrics['bertscore_precision'] = metrics['bertscore_recall'] = 0.0

        if SIMCSE_AVAILABLE and self.simcse_model is not None:
            try:
                embs = self.simcse_model.encode([ground_truth, prediction], convert_to_tensor=True)
                sim = util.cos_sim(embs[0], embs[1]).item()
                metrics['simcse'] = float(sim)
            except Exception as e:
                print(f"  âš ï¸ SimCSEè®¡ç®—å¤±è´¥: {e}")
                metrics['simcse'] = 0.0
        else:
            metrics['simcse'] = 0.0

        return metrics

    def evaluate_corrected_text(self, gt_errors: List[Dict[str, Any]],
                                pred_errors: List[Dict[str, Any]]) -> Dict[str, float]:
        """è¯„ä¼°ä¿®æ­£æ–‡æœ¬"""
        if not BERTSCORE_AVAILABLE and not (SIMCSE_AVAILABLE and self.simcse_model is not None):
            return {
                'bertscore_f1': 0.0,
                'bertscore_precision': 0.0,
                'bertscore_recall': 0.0,
                'simcse': 0.0,
            }
        if not gt_errors:
            return {
                'bertscore_f1': 1.0,
                'bertscore_precision': 1.0,
                'bertscore_recall': 1.0,
                'simcse': 1.0,
            }

        def normalize_text(text: str) -> str:
            if not text:
                return ''
            return re.sub(r'[\sï¼Œã€‚ã€ï¼›ï¼šï¼ï¼Ÿ""\'ï¼ˆï¼‰ã€ã€‘]', '', text)

        gt_sentences, pred_sentences = [], []
        for err in gt_errors:
            loc = err.get('error_location', '') or err.get('context', '')
            if loc and loc.strip():
                gt_sentences.append(loc.strip())
        for err in pred_errors:
            loc = err.get('error_location', '') or err.get('context', '')
            if loc and loc.strip():
                pred_sentences.append(loc.strip())

        matched_pairs: List[tuple[int, int]] = []
        matched_pred = set()
        for i, gt_sent in enumerate(gt_sentences):
            gt_norm = normalize_text(gt_sent)
            best_idx, best_sim = -1, 0.0
            for j, pred_sent in enumerate(pred_sentences):
                if j in matched_pred:
                    continue
                pred_norm = normalize_text(pred_sent)
                is_contained = gt_norm in pred_norm or pred_norm in gt_norm
                if gt_norm == pred_norm:
                    sim = 1.0
                elif is_contained:
                    sim = 1.0
                else:
                    gt_chars, pred_chars = set(gt_norm), set(pred_norm)
                    if gt_chars or pred_chars:
                        inter = len(gt_chars & pred_chars)
                        union = len(gt_chars | pred_chars)
                        sim = inter / union if union > 0 else 0.0
                    else:
                        sim = 0.0
                if (is_contained or sim > 0.8) and sim > best_sim:
                    best_sim, best_idx = sim, j
            if best_idx >= 0:
                matched_pairs.append((i, best_idx))
                matched_pred.add(best_idx)

        all_precisions: List[float] = []
        all_recalls: List[float] = []
        all_f1s: List[float] = []
        all_simcse: List[float] = []

        for gt_idx, pred_idx in matched_pairs:
            gt_correct = gt_errors[gt_idx].get('correct_context', '') or gt_errors[gt_idx].get('correct_content', '')
            pred_correct = pred_errors[pred_idx].get('correct_content', '') or pred_errors[pred_idx].get('correct_context', '')
            if not gt_correct or not pred_correct:
                continue

            if BERTSCORE_AVAILABLE:
                try:
                    P, R, F1 = bert_score([pred_correct], [gt_correct], lang='zh', verbose=False)
                    all_precisions.append(float(P[0].item()))
                    all_recalls.append(float(R[0].item()))
                    all_f1s.append(float(F1[0].item()))
                except Exception as e:
                    print(f"  âš ï¸ BERTScoreè®¡ç®—å¤±è´¥ï¼ˆä¿®æ­£æ–‡æœ¬ï¼‰: {e}")

            if SIMCSE_AVAILABLE and self.simcse_model is not None:
                try:
                    embs = self.simcse_model.encode([gt_correct, pred_correct], convert_to_tensor=True)
                    sim = util.cos_sim(embs[0], embs[1]).item()
                    all_simcse.append(float(sim))
                except Exception as e:
                    print(f"  âš ï¸ SimCSEè®¡ç®—å¤±è´¥ï¼ˆä¿®æ­£æ–‡æœ¬ï¼‰: {e}")

        if not all_f1s and not all_simcse:
            return {
                'bertscore_f1': 0.0,
                'bertscore_precision': 0.0,
                'bertscore_recall': 0.0,
                'simcse': 0.0,
            }

        bert_f1 = sum(all_f1s) / len(all_f1s) if all_f1s else 0.0
        bert_p = sum(all_precisions) / len(all_precisions) if all_precisions else 0.0
        bert_r = sum(all_recalls) / len(all_recalls) if all_recalls else 0.0
        simcse_avg = sum(all_simcse) / len(all_simcse) if all_simcse else 0.0

        return {
            'bertscore_f1': bert_f1,
            'bertscore_precision': bert_p,
            'bertscore_recall': bert_r,
            'simcse': simcse_avg,
            'num_evaluated_errors': max(len(all_f1s), len(all_simcse)),
        }

    def print_statistics(self, evaluation_results: List[Dict[str, Any]]):
        """æ‰“å°ç»Ÿè®¡ç»“æœ"""
        if not evaluation_results:
            print("  æ— è¯„ä¼°ç»“æœ")
            return

        n = len(evaluation_results)
        results_with_errors = [
            r for r in evaluation_results
            if r['error_list']['metrics']['num_gt_errors'] > 0
        ]
        n_with_errors = len(results_with_errors)

        sentence_f1s = [r['error_list']['metrics']['sentence_f1'] for r in results_with_errors]
        sentence_precisions = [r['error_list']['metrics']['sentence_precision'] for r in results_with_errors]
        sentence_recalls = [r['error_list']['metrics']['sentence_recall'] for r in results_with_errors]

        bertscore_f1s = [r['explanation']['metrics']['bertscore_f1'] for r in results_with_errors]
        simcse_explains = [r['explanation']['metrics']['simcse'] for r in results_with_errors]

        corrected_bertscore_f1s = [r['corrected_text']['metrics']['bertscore_f1'] for r in results_with_errors]
        simcse_corrected = [r['corrected_text']['metrics']['simcse'] for r in results_with_errors]

        # LLMè¯„åˆ¤åˆ†æ•°
        llm_explanation_scores = []
        llm_correction_scores = []
        
        for r in results_with_errors:
            judge = r.get('llm_judge_eval')
            if judge and isinstance(judge, dict):
                reasoning = judge.get('error_reasoning', {})
                if reasoning:
                    scores = [
                        float(reasoning.get('correctness', {}).get('score', 0)),
                        float(reasoning.get('reasoning_rigor', {}).get('score', 0)),
                        float(reasoning.get('relevance', {}).get('score', 0)),
                        float(reasoning.get('completeness', {}).get('score', 0))
                    ]
                    llm_explanation_scores.append(sum(scores) / 4.0)
                
                correction = judge.get('corrected_text', {})
                if correction:
                    scores = [
                        float(correction.get('correctness', {}).get('score', 0)),
                        float(correction.get('reasoning_rigor', {}).get('score', 0)),
                        float(correction.get('relevance', {}).get('score', 0)),
                        float(correction.get('completeness', {}).get('score', 0))
                    ]
                    llm_correction_scores.append(sum(scores) / 4.0)

        print(f"\n{'='*80}")
        print("æ€»ä½“è¯„ä¼°ç»“æœ")
        print(f"{'='*80}")
        print(f"\næ ·æœ¬æ€»æ•°: {n}")
        print(f"æœ‰é”™è¯¯çš„æ ·æœ¬æ•°: {n_with_errors}")
        print(f"æ— é”™è¯¯çš„æ ·æœ¬æ•°: {n - n_with_errors}")

        if n_with_errors > 0:
            print(f"\nã€1. é”™è¯¯åˆ—ä¸¾ã€‘ï¼ˆä»…ç»Ÿè®¡æœ‰é”™è¯¯çš„æ ·æœ¬ï¼Œå…±{n_with_errors}ä¸ªï¼‰")
            print(f"  å¥å­çº§ Precision: {sum(sentence_precisions)/n_with_errors:.4f}")
            print(f"  å¥å­çº§ Recall: {sum(sentence_recalls)/n_with_errors:.4f}")
            print(f"  å¥å­çº§ F1: {sum(sentence_f1s)/n_with_errors:.4f}")

            print(f"\nã€2. é”™è¯¯è§£é‡Šã€‘ï¼ˆä»…ç»Ÿè®¡æœ‰é”™è¯¯çš„æ ·æœ¬ï¼Œå…±{n_with_errors}ä¸ªï¼‰")
            print(f"  BERTScore-F1: {sum(bertscore_f1s)/n_with_errors:.4f}")
            print(f"  SimCSE: {sum(simcse_explains)/n_with_errors:.4f}")
            if llm_explanation_scores:
                print(f"  LLM as a Judge: {sum(llm_explanation_scores)/len(llm_explanation_scores):.4f}")

            print(f"\nã€3. ä¿®æ­£æ–‡æœ¬ã€‘ï¼ˆä»…ç»Ÿè®¡æœ‰é”™è¯¯çš„æ ·æœ¬ï¼Œå…±{n_with_errors}ä¸ªï¼‰")
            print(f"  BERTScore-F1: {sum(corrected_bertscore_f1s)/n_with_errors:.4f}")
            print(f"  SimCSE: {sum(simcse_corrected)/n_with_errors:.4f}")
            if llm_correction_scores:
                print(f"  LLM as a Judge: {sum(llm_correction_scores)/len(llm_correction_scores):.4f}")
        else:
            print(f"\nã€1-3. é”™è¯¯æ£€æµ‹ç›¸å…³æŒ‡æ ‡ã€‘")
            print(f"  æ‰€æœ‰æ ·æœ¬å‡æ— é”™è¯¯ï¼Œè·³è¿‡é”™è¯¯åˆ—ä¸¾ã€é”™è¯¯è§£é‡Šã€ä¿®æ­£æ–‡æœ¬çš„è¯„ä¼°")
        print(f"{'='*80}\n")

    def evaluate_dataset(self, qa_file: str, output_file: Optional[str] = None, max_qa: int = None) -> List[Dict[str, Any]]:
        """è¯„ä¼°æ•°æ®é›†"""
        print("=" * 80)
        print("å¼€å§‹è¯„ä¼°æ•°æ®é›†")
        print("=" * 80)
        print(f"\nQA æ–‡ä»¶: {qa_file}")

        qa_dataset = self.load_qa_dataset(qa_file)
        if max_qa:
            qa_dataset = qa_dataset[:max_qa]
            print(f"âœ“ é™åˆ¶è¯„ä¼°æ•°é‡: {max_qa} æ¡")
        
        print(f"âœ“ åŠ è½½ QA æ•°æ®é›†: {len(qa_dataset)} æ¡")

        processed_qa_ids = set()
        if output_file and os.path.exists(output_file):
            os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else ".", exist_ok=True)
            try:
                with open(output_file, "r", encoding="utf-8") as f:
                    for line in f:
                        if line.strip():
                            try:
                                saved_item = json.loads(line.strip())
                                qa_id = saved_item.get("qa_id") or saved_item.get("evaluation", {}).get("qa_id")
                                if qa_id:
                                    processed_qa_ids.add(qa_id)
                            except json.JSONDecodeError:
                                continue
                print(f"âœ“ æ£€æµ‹åˆ°å·²æœ‰ç»“æœæ–‡ä»¶ï¼Œå·²å¤„ç† {len(processed_qa_ids)} ä¸ªæ ·æœ¬ï¼Œå°†è·³è¿‡è¿™äº›æ ·æœ¬")
            except Exception as e:
                print(f"âš ï¸ è¯»å–å·²æœ‰ç»“æœæ–‡ä»¶æ—¶å‡ºé”™: {e}ï¼Œå°†ä»å¤´å¼€å§‹å¤„ç†")
        elif output_file:
            os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else ".", exist_ok=True)
        
        print(f"\næœ€ç»ˆç»“æœå°†ä¿å­˜åˆ°: {output_file}")

        evaluation_results: List[Dict[str, Any]] = []
        full_results_to_save: List[Dict[str, Any]] = []
        skipped_count = 0

        for idx, qa_item in enumerate(qa_dataset, 1):
            qa_id = qa_item.get("qa_id", f"unknown_{idx}")
            question = qa_item.get("question", "")
            answer = qa_item.get("answer", {})
    
            
            print(f"\n[{idx}/{len(qa_dataset)}] å¤„ç†æ ·æœ¬: {qa_id}")
            
            # è°ƒç”¨æ¨¡å‹
            model_result = self.call_model(question)
            if model_result is None:
                print(f"  âœ— æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œè·³è¿‡è¯¥æ ·æœ¬")
                continue
            
            # æå–ground truthå’Œprediction
            gt_error_exists = answer.get("logic_error_exists", "") or answer.get("factual_error_exists", "") or answer.get(
                "semantic_inconsistency_exists", "å¦"
            )
            if not gt_error_exists:
                gt_error_exists = "å¦"
            gt_errors =answer.get("logic_errors", []) or answer.get("factual_errors", []) or answer.get("semantic_inconsistencies", [])
            gt_explanation = answer.get("error_explanation", "") or answer.get(
                "inconsistency_explanation", ""
            )

            pred_error_exists = model_result.get("error_exists", "å¦")
            pred_errors = model_result.get("errors", [])
            pred_explanation = model_result.get("explanation", "")

            has_errors = gt_error_exists in ['æ˜¯', 'yes', 'Yes', 'YES', '1', 'true', 'True'] or len(gt_errors) > 0

            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡ - ä¼ ç»ŸæŒ‡æ ‡
            if has_errors:
                error_list_metrics = self.evaluate_error_list(gt_errors, pred_errors)
                explanation_metrics = self.evaluate_explanation(gt_explanation, pred_explanation)
                corrected_text_metrics = self.evaluate_corrected_text(gt_errors, pred_errors)
            else:
                error_list_metrics = {
                    'sentence_f1': 0.0,
                    'sentence_precision': 0.0,
                    'sentence_recall': 0.0,
                    'num_gt_errors': 0,
                    'num_pred_errors': len(pred_errors),
                }
                explanation_metrics = {
                    'bertscore_f1': 0.0,
                    'bertscore_precision': 0.0,
                    'bertscore_recall': 0.0,
                    'simcse': 0.0,
                }
                corrected_text_metrics = {
                    'bertscore_f1': 0.0,
                    'bertscore_precision': 0.0,
                    'bertscore_recall': 0.0,
                    'simcse': 0.0,
                    'num_evaluated_errors': 0,
                }
                
            # LLM-as-a-Judge è¯„ä¼°
            judge_evaluation = None
            if self.judge_model:
                # åªæœ‰å½“Ground Truthå­˜åœ¨æˆ–æœ‰è¶³å¤Ÿä¿¡æ¯æ—¶æ‰è¿›è¡Œè£åˆ¤
                if gt_errors or gt_explanation or answer.get("corrected_text", ""):
                    try:
                        print(f"  ğŸ¤– è°ƒç”¨è£åˆ¤æ¨¡å‹ ({self.judge_model}) è¯„ä¼°...")
                        
                        # æ„å»ºæ¨¡å‹è¾“å‡ºå…¨é›†
                        model_full = {
                            "error_exists": pred_error_exists,
                            "errors": pred_errors,
                            "explanation": pred_explanation,
                            "corrected_text": model_result.get("corrected_text", "")
                        }
                        
                        # æ„å»ºå‚è€ƒç­”æ¡ˆå…¨é›†
                        ref_full = {
                            "factual_errors": gt_errors,
                            "semantic_inconsistencies": [], # ä¿æŒç»“æ„ä¸€è‡´æ€§
                            "error_explanation": gt_explanation,
                            "inconsistency_explanation": "",
                            "corrected_text": answer.get("corrected_text", "")
                        }
                        
                        prompt = self.build_judge_prompt(model_full, ref_full)
                        judge_evaluation = self.call_judge_model(prompt)
                        if judge_evaluation:
                            print(f"  âœ“ è£åˆ¤æ¨¡å‹è¯„ä¼°å®Œæˆ")
                        else:
                            print(f"  âš ï¸ è£åˆ¤æ¨¡å‹è¯„ä¼°è¿”å›ä¸ºç©º")
                            
                    except Exception as e:
                        print(f"  âš ï¸ è£åˆ¤æ¨¡å‹è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")

            evaluation_result = {
                "qa_id": qa_id,
                "has_errors": has_errors,
                "error_list": {
                    "ground_truth_count": len(gt_errors),
                    "prediction_count": len(pred_errors),
                    "metrics": error_list_metrics,
                },
                "explanation": {"metrics": explanation_metrics},
                "corrected_text": {"metrics": corrected_text_metrics},
                "llm_judge_eval": judge_evaluation
            }
            
            evaluation_results.append(evaluation_result)
            result_item = {
                "qa_id": qa_id,
                "model_result": model_result,
                "evaluation": evaluation_result
            }
            full_results_to_save.append(result_item)
            print(f"  âœ“ å®Œæˆè¯„ä¼°")

        # ç»“æŸåç»Ÿä¸€ä¿å­˜ä¸ºJSON
        if output_file:
            try:
                with open(output_file, "w", encoding="utf-8") as f:
                    json.dump(full_results_to_save, f, ensure_ascii=False, indent=2)
                print(f"  âœ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³JSONæ–‡ä»¶: {output_file}")
            except Exception as e:
                print(f"  âœ— ä¿å­˜ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
                print(f"  âœ“ å®Œæˆè¯„ä¼°")

        # æ‰“å°å¤„ç†æ‘˜è¦
        total_processed = len(evaluation_results)
        print(f"\n{'='*80}")
        print(f"å¤„ç†å®Œæˆæ‘˜è¦:")
        print(f"  æ€»æ ·æœ¬æ•°: {len(qa_dataset)}")
        print(f"  è·³è¿‡æ ·æœ¬æ•°: {skipped_count}")
        print(f"  æ–°å¤„ç†æ ·æœ¬æ•°: {total_processed}")
        print(f"{'='*80}")

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self.print_statistics(evaluation_results)
        
        return evaluation_results


def run(config):
    """
    ç»Ÿä¸€å…¥å£å‡½æ•°
    """
    api_key = config.get('api_key')
    base_url = config.get('base_url')
    model = config.get('model', 'deepseek-chat')
    judge_model = config.get('judge_model')
    
    input_path = config.get('input_path')
    if not input_path:
        input_path = os.path.join("data", "Consistency")
    output_dir = config.get('output_dir', 'eval_results/consistency')
    
    files_to_process = []
    expected_files = [
        "Terminology_Evaluation.json", 
        "Fact_Evaluation.json", 
        "Logic_Evaluation.json"
    ]
    
    if os.path.isdir(input_path):
        found_expected = False
        for expected_name in expected_files:
            full_path = os.path.join(input_path, expected_name)
            if os.path.exists(full_path):
                files_to_process.append(full_path)
                found_expected = True
        
        if not found_expected:
            files = sorted([f for f in os.listdir(input_path) if f.endswith('.json') or f.endswith('.jsonl')])
            for f in files:
                files_to_process.append(os.path.join(input_path, f))
                
    elif os.path.isfile(input_path):
        files_to_process.append(input_path)
    else:
        if os.path.exists("Consistency"):
            found_expected = False
            for expected_name in expected_files:
                full_path = os.path.join("Consistency", expected_name)
                if os.path.exists(full_path):
                    files_to_process.append(full_path)
                    found_expected = True
            
            if not found_expected:
                for f in os.listdir("Consistency"):
                    if f.endswith('.json') or f.endswith('.jsonl'):
                        files_to_process.append(os.path.join("Consistency", f))
        else:
            logger.error(f"æœªæ‰¾åˆ°è¾“å…¥è·¯å¾„: {input_path}")
            return

    if not files_to_process:
        logger.warning(f"åœ¨ {input_path} ä¸­æœªæ‰¾åˆ°å¯å¤„ç†çš„ json/jsonl æ–‡ä»¶")
        return

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    evaluator = ErrorDetectionEvaluator(api_key=api_key, base_url=base_url, model=model, judge_model=judge_model)

    for input_file in files_to_process:
        dataset_name = Path(input_file).stem
        output_file = os.path.join(output_dir, f"{dataset_name}_results.json")
        evaluator.evaluate_dataset(qa_file=input_file, output_file=output_file, max_qa=config.get('max_qa'))

